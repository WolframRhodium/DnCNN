function lgraph = resnet(patchSize, channels, networkDepth)
% ResNet for denoising
% The depth of the network should be even
% Reference:
% [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). 
%   Deep residual learning for image recognition.
%   In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
    if networkDepth <= 0 || bitand(networkDepth, 1)
        error('resnet: "networkDepth" must be positive and even.')
    end
    
    layers = [];
    b_min = 0.025;
    
    Input = imageInputLayer([patchSize patchSize channels], ...
        'Normalization', 'none', ...
        'Name', 'Input');

    layers = [layers, Input];

    Conv = convolution2dLayer(3, 64, ...
            'Stride', 1, ...
            'Padding', 'same', ...
            'WeightLearnRateFactor', 1, ...
            'WeightL2Factor', 1, ...
            'BiasLearnRateFactor', 1, ...
            'BiasL2Factor', 0, ...
            'Name', 'Conv1');
    Conv.Weights = sqrt(2/(9*channels))*randn(3,3,channels,64,'single');
    Conv.Bias = zeros(1,1,64,'single');

    Relu = reluLayer(...
            'Name', 'ReLU1');

    layers = [layers, Conv, Relu];

    lgraph = layerGraph(layers);

    for i = 2:2:networkDepth-2
        % Residual unit
        Conv = convolution2dLayer(3, 64, ...
            'Stride', 1, ...
            'Padding', 'same', ...
            'WeightLearnRateFactor', 1, ...
            'WeightL2Factor', 1, ...
            'BiasLearnRateFactor', 0, ...
            'BiasL2Factor', 0, ...
            'Name', ['Conv', num2str(i)]);
        Conv.Weights = sqrt(2/(9*64))*randn(3,3,64,64,'single');
        Conv.Bias = zeros(1,1,64,'single');

        Bnorm = batchNormalizationLayer(...
            'Scale', clipping(sqrt(2/(9*64))*randn(1,1,64,'single'), b_min), ...
            'ScaleLearnRateFactor', 1, ...
            'ScaleL2Factor', 0, ...
            'Offset', zeros(1,1,64,'single'), ...
            'OffsetLearnRateFactor', 1, ...
            'OffsetL2Factor', 0, ...
            'Name', ['BNorm', num2str(i)]);

        Relu = reluLayer(...
            'Name', ['ReLU', num2str(i)]);
        
        lgraph = addLayers(lgraph, Conv);
        lgraph = addLayers(lgraph, Bnorm);
        lgraph = addLayers(lgraph, Relu);

        if i == 2
            lgraph = connectLayers(lgraph, 'ReLU1', 'Conv2');
        else
            lgraph = connectLayers(lgraph, ['Add', num2str(i-1)], ['Conv', num2str(i)]);
        end
        lgraph = connectLayers(lgraph, ['Conv', num2str(i)], ['BNorm', num2str(i)]);
        lgraph = connectLayers(lgraph, ['BNorm', num2str(i)], ['ReLU', num2str(i)]);
        
        Conv = convolution2dLayer(3, 64, ...
            'Stride', 1, ...
            'Padding', 'same', ...
            'WeightLearnRateFactor', 1, ...
            'WeightL2Factor', 1, ...
            'BiasLearnRateFactor', 0, ...
            'BiasL2Factor', 0, ...
            'Name', ['Conv', num2str(i+1)]);
        Conv.Weights = sqrt(2/(9*64))*randn(3,3,64,64,'single');
        Conv.Bias = zeros(1,1,64,'single');

        Bnorm = batchNormalizationLayer(...
            'Scale', clipping(sqrt(2/(9*64))*randn(1,1,64,'single'), b_min), ...
            'ScaleLearnRateFactor', 1, ...
            'ScaleL2Factor', 0, ...
            'Offset', zeros(1,1,64,'single'), ...
            'OffsetLearnRateFactor', 1, ...
            'OffsetL2Factor', 0, ...
            'Name', ['BNorm', num2str(i+1)]);

        Relu = reluLayer(...
            'Name', ['ReLU', num2str(i+1)]);
        
        lgraph = addLayers(lgraph, Conv);
        lgraph = addLayers(lgraph, Bnorm);
        lgraph = addLayers(lgraph, Relu);
        
        lgraph = connectLayers(lgraph, ['ReLU', num2str(i)], ['Conv', num2str(i+1)]);
        lgraph = connectLayers(lgraph, ['Conv', num2str(i+1)], ['BNorm', num2str(i+1)]);
        lgraph = connectLayers(lgraph, ['BNorm', num2str(i+1)], ['ReLU', num2str(i+1)]);

        Add = additionLayer(2, ...
            'Name', ['Add', num2str(i+1)]);

        lgraph = addLayers(lgraph, Add);
        
        if i == 2
            lgraph = connectLayers(lgraph, 'ReLU1', 'Add3/in1');
        else
            lgraph = connectLayers(lgraph, ['Add', num2str(i-1)], ['Add', num2str(i+1), '/in1']);
        end
        lgraph = connectLayers(lgraph, ['ReLU', num2str(i+1)], ['Add', num2str(i+1), '/in2']);
    end

    Conv = convolution2dLayer(3, channels, ...
            'Stride', 1, ...
            'Padding', 'same', ...
            'WeightLearnRateFactor', 1, ...
            'WeightL2Factor', 1, ...
            'BiasLearnRateFactor', 1, ...
            'BiasL2Factor', 0, ...
            'Name', ['Conv', num2str(networkDepth)]);
    Conv.Weights = sqrt(2/(9*64))*randn(3,3,64,channels,'single');
    Conv.Bias = zeros(1,1,channels,'single');

    lgraph = addLayers(lgraph, Conv);

    if networkDepth == 2
        lgraph = connectLayers(lgraph, 'ReLU1', 'Conv2');
    else % networkDepth >= 4
        lgraph = connectLayers(lgraph, ['Add', num2str(networkDepth-1)], ['Conv', num2str(networkDepth)]);
    end

    Regression = regressionLayer(...
            'Name', 'Output');

    lgraph = addLayers(lgraph, Regression);

    lgraph = connectLayers(lgraph, ['Conv', num2str(networkDepth)], 'Output');
    
    % plot(lgraph);
end

function A = clipping(A, b)
    A(A>=0 & A<b) = b;
    A(A<0 & A>-b) = -b;
end
